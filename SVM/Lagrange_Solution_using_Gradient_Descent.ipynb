{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SVM Solution using Gradient Descent\n",
        "\n",
        "**Uzair Ahmad**\n"
      ],
      "metadata": {
        "id": "c3WycXrxnwjZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Col5eerfiaTW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Gram Matrix:\n",
        "\n",
        "the inner-product of feature vectors.\n",
        "The inner products represent the similarity between feature vectors"
      ],
      "metadata": {
        "id": "cuEcYTlQZs3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[2, 2],[2, -1],[-2,-2],[-2,1]])\n",
        "N = X.shape[0]\n",
        "X = normalize(X)\n",
        "print(X)\n",
        "Xi_Xj = np.round(np.dot(X, X.T))\n",
        "Xi_Xj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2EBq5vwi8Rd",
        "outputId": "a921fef3-2396-4e50-d1be-9dde86b07222"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.70710678  0.70710678]\n",
            " [ 0.89442719 -0.4472136 ]\n",
            " [-0.70710678 -0.70710678]\n",
            " [-0.89442719  0.4472136 ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.,  0., -1., -0.],\n",
              "       [ 0.,  1., -0., -1.],\n",
              "       [-1., -0.,  1.,  0.],\n",
              "       [-0., -1.,  0.,  1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The class labels\n",
        "The $M=Y_iY_j$ matrix represents the pairwise relationships between the class labels. Each element of $M$ indicates whether two corresponding elements in the original label vector $y$ belong to the same class (1) or different classes (-1)."
      ],
      "metadata": {
        "id": "susW8LVOZUWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.array([1,1, -1,-1])\n",
        "\n",
        "Yi_Yj = np.outer(y, y)\n",
        "Yi_Yj"
      ],
      "metadata": {
        "id": "M67AGXp-igmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1041ed2a-a1a6-4290-8578-5a7d1ad81fb9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  1, -1, -1],\n",
              "       [ 1,  1, -1, -1],\n",
              "       [-1, -1,  1,  1],\n",
              "       [-1, -1,  1,  1]])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- By incorporating the class label information through the multiplication with $M$, the SVM's decision function may adapt to the pairwise relationships between classes, potentially leading to decision boundaries that better separate different classes in the feature space."
      ],
      "metadata": {
        "id": "wrtvWsk9aUf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G = Yi_Yj * Xi_Xj\n",
        "G"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJeHgtycjjFp",
        "outputId": "88fb0f7c-160a-4da8-9971-a4d7c81d6023"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 1., 0.],\n",
              "       [0., 1., 0., 1.],\n",
              "       [1., 0., 1., 0.],\n",
              "       [0., 1., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Initializtion\n",
        "  - alphas: Initializes the Lagrange multipliers to zeros. These multipliers are the variables being optimized in the dual problem of the SVM.\n",
        "  - ones: Creates an array of ones with the same size as the number of samples in the dataset.\n",
        "  - lr: Defines the learning rate for the gradient ascent algorithm.\n",
        "  - C: Represents the regularization parameter in the SVM formulation."
      ],
      "metadata": {
        "id": "SkdIvBwcbBoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize alphas as zeros\n",
        "alphas = np.zeros(N)\n",
        "\n",
        "# Constants\n",
        "ones = np.ones(N)\n",
        "lr = 0.1\n",
        "C = 1"
      ],
      "metadata": {
        "id": "I1Lp_9MMbOL2"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient of Dual Formulation of SVM"
      ],
      "metadata": {
        "id": "tbQIsfOKo99A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's derive the gradient of the objective function with respect to the Lagrange multipliers in the context of the SVM dual optimization problem.\n",
        "\n",
        "In the dual formulation of SVMs, the objective function to be maximized is typically expressed as follows:\n",
        "\n",
        "$\n",
        "L(\\boldsymbol{\\alpha}) = \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $ \\boldsymbol{\\alpha} $ is the vector of Lagrange multipliers.\n",
        "- $ N $ is the number of samples in the dataset.\n",
        "- $ y_i $ and $ y_j $ are the class labels of samples $ i $ and $ j $.\n",
        "- $ K(\\mathbf{x}_i, \\mathbf{x}_j) $ is the kernel function evaluating the inner product between feature vectors $ \\mathbf{x}_i $ and $ \\mathbf{x}_j$.\n",
        "\n",
        "The gradient of the objective function $ L(\\boldsymbol{\\alpha}) $ with respect to the Lagrange multipliers $ \\boldsymbol{\\alpha} $ can be computed as follows:\n",
        "\n",
        "$\n",
        "\\frac{\\partial L(\\boldsymbol{\\alpha})}{\\partial \\alpha_i} = 1 - y_i \\sum_{j=1}^{N} \\alpha_j y_j K(\\mathbf{x}_i, \\mathbf{x}_j)\n",
        "$\n",
        "\n",
        "Where $ i = 1, 2, ..., N $.\n",
        "\n",
        "Here's the derivation:\n",
        "\n",
        "1. The derivative of the first term $ \\sum_{i=1}^{N} \\alpha_i $ with respect to $ \\alpha_i $ is simply 1.\n",
        "\n",
        "2. For the second term $ -\\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j) $, we can focus on the part involving $ \\alpha_i $. Taking the derivative with respect to $ \\alpha_i $ yields:\n",
        "\n",
        "$\n",
        "\\frac{\\partial}{\\partial \\alpha_i} \\left( -\\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j) \\right) = -y_i \\sum_{j=1}^{N} \\alpha_j y_j K(\\mathbf{x}_i, \\mathbf{x}_j)\n",
        "$\n",
        "\n",
        "This is because when $ j $ runs over all indices, only $ \\alpha_i $ comes out as a constant, hence the summation is left over.\n",
        "\n",
        "3. Finally, the total gradient is the sum of gradients of each term, leading to:\n",
        "\n",
        "$\n",
        "\\frac{\\partial L(\\boldsymbol{\\alpha})}{\\partial \\alpha_i} = 1 - y_i \\sum_{j=1}^{N} \\alpha_j y_j K(\\mathbf{x}_i, \\mathbf{x}_j)\n",
        "$\n"
      ],
      "metadata": {
        "id": "D0vsrusmdJcg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The main loop"
      ],
      "metadata": {
        "id": "5y3_sAt8pEz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main loop\n",
        "for _ in range(10):\n",
        "    # Compute the gradient\n",
        "    gradient = ones - np.dot(Yi_Yj, alphas)\n",
        "\n",
        "    # Update alphas\n",
        "    alphas += lr * gradient\n",
        "\n",
        "    # Clip alphas to be between 0 and C\n",
        "    #alphas = np.clip(alphas, 0, C)\n",
        "    alphas[alphas > C] = C\n",
        "    alphas[alphas < 0] = 0\n",
        "    # Compute the objective function\n",
        "    gain = np.sum(alphas) - 0.5 * np.sum(np.outer(alphas, alphas) * G)\n",
        "    print(gain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8P_8_yJay6V",
        "outputId": "b5c5d2c4-bae6-40d1-c7af-0fd15f90b429"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.36\n",
            "0.64\n",
            "0.8400000000000001\n",
            "0.96\n",
            "1.0\n",
            "0.96\n",
            "0.8400000000000001\n",
            "0.6400000000000001\n",
            "0.3600000000000003\n",
            "4.440892098500626e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying Support Vectors: extracts the indices of non-zero Lagrange multipliers, which correspond to the support vectors in the SVM solution."
      ],
      "metadata": {
        "id": "cc3g5FrTeDI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the indices of non-zero alphas (support vectors)\n",
        "index = np.where(alphas > 0)[0]\n",
        "index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5fNJTvjeCMY",
        "outputId": "aa7027d1-c600-41e3-96bb-b6bb06b09f03"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xi_Xs = np.dot(X, X[index].T)\n",
        "Xi_Xs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rlRQ9EsvFfQ",
        "outputId": "d3b47b92-6c7d-4b23-86bc-076fbc8b61e0"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.        ,  0.31622777, -1.        , -0.31622777],\n",
              "       [ 0.31622777,  1.        , -0.31622777, -1.        ],\n",
              "       [-1.        , -0.31622777,  1.        ,  0.31622777],\n",
              "       [-0.31622777, -1.        ,  0.31622777,  1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The $w$s of decision boundary\n",
        "To calculate the weights of the linear boundary in a linear SVM, we typically use the formula derived from the support vectors and their corresponding Lagrange multipliers.\n",
        "\n",
        "In the dual formulation of linear SVMs, the decision boundary can be expressed as:\n",
        "\n",
        "$f(x)=∑_{i=1}^Nα_iy_i⟨x_i,x⟩+b$\n",
        "\n",
        "To calculate the weights ww of the linear boundary, you take the sum of the Lagrange multipliers αiyiαi​yi​ times their corresponding support vectors $x_i$​:\n",
        "\n",
        "$w=\\sum_{i=1}^Nα_iy_ix_i$\n",
        "\n",
        "In code, this can be represented as:"
      ],
      "metadata": {
        "id": "ZBuGexm_qfhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the weights of the linear boundary\n",
        "w = np.dot((alphas * y)[index], X[index])\n",
        "w"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3hhZoWqrIQD",
        "outputId": "e09e9c7c-eccc-4759-bbfe-12a9791227ba"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3.20306794, 0.51978637])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The bias"
      ],
      "metadata": {
        "id": "wb_mYMRIp_cj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the mean of the differences between the class labels of support vectors and the similarities or distances of all samples to the support vectors. This mean value represents the bias term bb for the SVM decision function."
      ],
      "metadata": {
        "id": "vkBG0HBQffei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b_i = y[index] - (alphas * y).dot(np.dot(X, X[index].T))\n",
        "b = np.mean(b_i)\n",
        "b"
      ],
      "metadata": {
        "id": "YgNMfz8pvtqv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce262681-4493-472f-ff09-5952cb24ef07"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The decision boundary\n",
        "\n",
        "While the expression itself is not explicitly the decision boundary equation, it represents the decision function that determines the boundary between classes in SVM classification. It evaluates the sign of the function to classify input vectors into respective classes.\n",
        "\n",
        "```python\n",
        "np.sign((alphas * y).dot(np.dot(X, X[index].T)) + b)\n",
        "```\n",
        "\n",
        "is computing the sign of the decision function. In SVMs, the decision function is defined as:\n",
        "\n",
        "$ f(\\mathbf{x}) = \\text{sign}\\left(\\sum_{i=1}^{N} \\alpha_i y_i \\mathbf{x}_i, \\mathbf{x} + b\\right) $\n",
        "\n",
        "where:\n",
        "- $\\alpha_i $ are the Lagrange multipliers.\n",
        "- $ y_i $ are the class labels.\n",
        "- $ \\mathbf{x}_i, \\mathbf{x} $ is the inner-product of features .\n",
        "- $ b $ is the bias term.\n",
        "\n",
        "Here's how the provided expression corresponds to the decision function:\n",
        "\n",
        "1. **Computing Inner Product:**\n",
        "   ```python\n",
        "   (alphas * y).dot(np.dot(X, X[index].T))\n",
        "   ```\n",
        "   This part computes the inner product between the feature vectors $ \\mathbf{x} $ and the support vectors, weighted by the Lagrange multipliers and class labels.\n",
        "\n",
        "2. **Adding Bias:**\n",
        "   ```python\n",
        "   (alphas * y).dot(np.dot(X, X[index].T)) + b\n",
        "   ```\n",
        "   The bias term $ b $ is added to the result of the inner product. This adjusts the decision boundary based on the location of the support vectors and their corresponding class labels.\n",
        "\n",
        "3. **Taking the Sign:**\n",
        "   ```python\n",
        "   np.sign((alphas * y).dot(np.dot(X, X[index].T)) + b)\n",
        "   ```\n",
        "   The `np.sign()` function returns -1 if the argument is negative, 0 if it's zero, and +1 if it's positive. This determines the class label assigned by the SVM for a given input vector $ \\mathbf{x} $.\n",
        "\n",
        "Therefore, the expression calculates the sign of the decision function, which determines the predicted class label for a given input vector $ \\mathbf{x} $. If the result is positive, it means the input vector belongs to one class, and if it's negative, it belongs to the other class.\n"
      ],
      "metadata": {
        "id": "TsxF1nQZfm3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.sign((alphas * y).dot(np.dot(X, X[index].T)) + b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMSyQ1Xtwzhr",
        "outputId": "6b0b55d8-5be0-46b2-db85-0b55a6630e2e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.,  1., -1., -1.])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    }
  ]
}