'''
Author: Uzair Ahmad
Date: 0250721
'''

from scipy.stats import multivariate_normal
import numpy as np

class ExpectationMaximizer:

    def __init__(self, X, k=2, random_state=1):
        """initialize an object of ExpectationMaximizer by
      1. initializing prior probabilities of each cluster as uniform distribution.
      2. randomly selecting K samples as initial cluster-centers
      3. initializing covariance matrix as
        Keyword arguments:
        K -- the number of clusters (default 0.0)
        X -- the design matrix n rows m columns
        """
        self.cluster_assignments = None
        self.posteriors = None
        self.K = k
        self.X = X
        self.N = X.shape[0]  # number of data points
        self.iteration = 0
        np.random.seed(random_state)

        # initialize prior probabilities as uniform distribution
        self.priors = np.full(shape=self.K, fill_value=1 / self.K)
        # randomly select K examples as mean of each cluster from dataset
        self.mu_estimates = [self.X[row_index, :] for row_index in
                             np.random.randint(low=0, high=self.N, size=self.K)]
        # covariance matrix estimate from dataset
        self.cov_estimates = [np.cov(self.X.T) for _ in range(self.K)]

        self.likelihood = np.zeros((self.N, self.K))
        for Zi in range(self.K):
            # calculates pdf w.r.t. corresponding mean and covariance
            distribution = multivariate_normal(mean=self.mu_estimates[Zi],
                                               cov=self.cov_estimates[Zi],
                                               allow_singular=True)
            # retrieves likelihood from the pdf
            self.likelihood[:, Zi] = distribution.pdf(self.X)  # P(X | Zi) --> Likelihood

    def E_step(self):
        """Performs the expectation step [ Calculates expected values P(Zi | X)]
        [If the model parameters (mu, cov) are true, the
                1. For each cluster,
                  update estimates of cluster-centers
                  update covariance matrix
                  update priors for each cluster
        """
        # Calculate P(Zi | X) and update model parameters (mu, cov) and priors
        # calculate join probability --> P(X, C) = P(X | Zi) * P(Zi)
        joint_prob = (self.likelihood * self.priors)  # P(X, Zi) = P(X | Zi) * P(Zi)
        # calculate total evidence (a.k.a marginal probability) --> P(X)
        marginal_prob = joint_prob.sum(axis=1)[:, np.newaxis]  # P(X)
        # calculate posterior (class conditional probabilities for each datapoint) --> P(Zi | X) = P(X,C)/P(X)
        # maximum-likelihood that X is generated by Zi
        self.posteriors = joint_prob / marginal_prob  # P(Zi | X)

    def M_step(self):
        """Calculate maximum likelihood - P(X | Zi)
          1. create an NxK matrix to store likelihood values --> P(X | Zi)
          2. For each cluster,
            calculates pdf w.r.t. corresponding mean and covariance
            retrieves likelihood from the pdf
          3. calculate joint probability --> P(X, Z) = P(X | Zi) * P(Zi)
          4. calculate total evidence (aka marginal probability)  --> P(X)
          5. calculate posterior (class conditional probabilities for each datapoint) --> P(Zi | X) = P(X,C)/P(X)
          6. Update priors
        """
        for Zi in range(self.K):
            # calculates pdf w.r.t. corresponding mean and covariance
            distribution = multivariate_normal(mean=self.mu_estimates[Zi],
                                               cov=self.cov_estimates[Zi],
                                               allow_singular=True)
            # retrieves likelihood from the pdf
            self.likelihood[:, Zi] = distribution.pdf(self.X)  # P(X | Zi) --> Likelihood

        x = self.likelihood.sum()
        # Likelihood of data points given theta (mean, cov)
        print('likelihood of X: ', x)

        # Update model parameters for each Zi
        for Zi in range(self.K):  # for each distribution (cluster)
            posterior_Zi = self.posteriors[:, [Zi]]  # P(Zi | X)
            # update mean estimate
            self.mu_estimates[Zi] = (self.X * posterior_Zi).sum(axis=0) / posterior_Zi.sum()
            # update covariance
            self.cov_estimates[Zi] = np.cov(self.X.T,
                                            aweights=posterior_Zi.flatten(),  # weight of each datapoint
                                            )
        # update cluster assignments
        self.cluster_assignments = np.argmax(self.posteriors, axis=1)
        # update priors
        self.priors = np.sum(self.posteriors, axis=0) / self.N

    def run(self, iterations=10):
        """Performs the Epectation-maximization iterations
        """
        for i in range(iterations):
            self.E_step()
            self.M_step()
        return self


X = np.array([
    [1, 2, 3, 4,  8, 9, 10, 11,  -1, -5, -7, -3, -2, -1, -4, -3, -2]
]).T
#
# # X = np.array([
# #     [1, 2, 3, 4, -1, -2, -3, -4, -1, -2, -3, -4, 1, 2, 3, 4],
# #     [1, 2, 3, 4,  1,  2,  3,  4, -1, -2, -3, -4, -1, -2, -3, -4]
# # ]).T
# change random_state to see the effect of initial-selection.
em = ExpectationMaximizer(X=X, k=2, random_state=2).run(10)
print(em.mu_estimates)
print(em.cluster_assignments)
print(em.priors)
#
# centroids = np.array(em.mu_estimates).argsort()
# print(centroids)
