# The gradient of log-likelihood

## Step-by-step derivation 

**Uzair Ahmad**

In logistic regression, the goal is to predict a binary outcome. The model provides a probabilistic estimate, which is then thresholded to make a binary decision. The most commonly used probability function is the logistic sigmoid function. To train the logistic regression model, we aim to find parameters (often denoted as $w$) that maximize the likelihood of the observed data, or equivalently, minimize the negative log-likelihood.

Given the probabilistic nature of logistic regression, it's natural to use the log-likelihood as the objective function. By differentiating this function with respect to the parameters, we can obtain the gradient, which is used in optimization algorithms like gradient descent to update the parameters and find the best fit for the data.

Let's review a step-by-step derivation of this gradient, starting from the logistic function, building up the log-likelihood, and culminating in the expression for the gradient of the negative log-likelihood with respect to the logistic regression parameters.

**1. Probabilistic Model**:

Given an input vector $x_i$, logistic regression models the probability that $y_i = 1$ as:

$p(y_i = 1 | x_i, w) = \frac{1}{1 + e^{-w^T x_i}}$

This can be represented as:
$$ p = \frac{1}{1 + e^{-z}} $$
where $z = w^T x_i$.

**2. Log-Likelihood**:

For a single data point, the log-likelihood is given by:
$$ l(w) = y_i \log(p) + (1 - y_i) \log(1-p) $$

**3. Objective Function**:

Often, in logistic regression, we maximize the log-likelihood across all samples, or equivalently, minimize the negative log-likelihood:

$ J(w) = - \sum_{i=1}^n [y_i \log(p_i) + (1 - y_i) \log(1-p_i)] $

**4. Gradient Derivation**:

To find the gradient, we'll differentiate $J(w)$ with respect to $w_j$. Let's focus on the derivative of the term for a single sample first:

$$ \frac{\partial}{\partial w_j} [-y_i \log(p) - (1 - y_i) \log(1-p)] $$

Using the chain rule:

$$ \frac{\partial J}{\partial w_j} = -y_i \frac{\partial \log(p)}{\partial p} \frac{\partial p}{\partial w_j} - (1 - y_i) \frac{\partial \log(1-p)}{\partial p} \frac{\partial p}{\partial w_j} $$

The derivatives for the log terms are straightforward:

$$ \frac{\partial \log(p)}{\partial p} = \frac{1}{p} $$
and
$$ \frac{\partial \log(1-p)}{\partial p} = -\frac{1}{1-p} $$

For the term $\frac{\partial p}{\partial w_j}$, using the derivative of the sigmoid function:

$$ \frac{\partial p}{\partial z} = p(1-p) $$

and 

$$ \frac{\partial z}{\partial w_j} = x_{ij} $$

Combining, we get:

$$ \frac{\partial p}{\partial w_j} = x_{ij} p(1-p) $$

**5. Combining the above results**:

Starting from the previous differentiation step:
$$ \frac{\partial J}{\partial w_j} = -y_i \frac{1}{p} x_{ij} p(1-p) + (1 - y_i) \frac{1}{1-p} x_{ij} p(1-p) $$

Expanding:

1. For the first term:
$ -y_i \frac{1}{p} x_{ij} p(1-p) $
$ = -y_i x_{ij} (1-p)$
$ = -y_i x_{ij} + y_i x_{ij} p $

2. For the second term:
$ (1 - y_i) \frac{1}{1-p} x_{ij} p(1-p) $
$ = (1 - y_i) x_{ij} p $
$ = x_{ij} p - y_i x_{ij} p $

Now, summing these two results together:
$ \frac{\partial J}{\partial w_j} = (-y_i x_{ij} + y_i x_{ij} p + x_{ij} p - y_i x_{ij} p) $
$ \frac{\partial J}{\partial w_j} = -y_i x_{ij} + x_{ij} p $

Grouping terms:
$ \frac{\partial J}{\partial w_j} = (p - y_i) x_{ij} $

Thus, the gradient of the loss function with respect to $w_j$ is $(p - y_i) x_{ij}$.

------

